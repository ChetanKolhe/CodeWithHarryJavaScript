Python Treading Tutorial
1965 : Gordon E. Moore
The number of transistors in a dense integrated circuit will continue to double approximately double every 2 years.

What concurrency: is the execution of multiple instruction sequence at the same time .

Order of Execution .
Share Resource .


Concurrency Types:
1. Parallel Programming . ( Best suitted by CPU intensive task )
	e.g
	String operations
	Search Algorithms
	Grpahic Processing
	Number Crunching
	Algorithms


2. Asynchronous Programming . ( Best suitted for I/O bound task )
	Delegating to task some other thread or actore and continue doing other work .
	When subtask get completed it note back to main therad , this is generally called
	callback function .

	Database reads , wirtes .
	Web Service calls
	Copying , downloading , uploading data .


Python support both parallel and Asynchronous Programming natively .
threading 1.5 , multiprocessing 2.6+

concurrent.futures 3.2
asyncio 3.4



Process :
The execution context of a running program .
or running instace of computer program .


Thread : The smallest sequence of instruction that can be managed by operating system .


How to create a Thread

import threading

def do_somework(val):
    print("doing some work in thread")
	print("Eval : {}".format(val))
    return

val = "text"
t = threading.Thread(target=do_some_work, args=(val,))
t.start()
t.join()


class threading.Thread(group=None,
                       target=None,
					   name=None,
					   args=(),
					   kwargs={},
					   daemon=None
					   )

A boolean value indicating whether this thread is a daemon thread (True) or not (False).
This must be set before start() is called, otherwise RuntimeError is raised.
Its initial value is inherited from the creating thread;
the main thread is not a daemon thread and therefore all threads created in the main thread default to daemon = False.


Creating thread by using extending thread module .

class myThread(theading.Thread):
	def __init__(self):
		super().__init__()    # Thread.__init__(self)   same thing can also be called in that way

	def run(self):
		print("This is running thread")

my_feb_task = myThread(1)
my_feb_task.start()
my_feb_task.join()



Thread Lifecycle

New --> Ready --> Running -->Terminate
			\		/
			 \	   /
			 Blocked


Once thread had started we have little control over it .

The Scheduler :
 an operating system module that select next jobs to be admitted into the
 system and the next process to run .


Context Switch :
the process of saving and restoring the state of a thread or process .

Context switching using process to expensive rather than thread .
This is one of the reason parallel programing using thread over thread .
Parallel processing over processing .


Thread Interferene .
If resource share between thread , then most probably thread interferance will occure .
Same thing also called as race condition .
Due to that may be data corruption also occur .



In multithreading , it is best practice to share minimum resource .



Thread Synchronization

threading>Lock

	locked or unlocked

	lock.acquire()
	lock.release()

example
	lock = threading.Lock()
	lock.acquire()
	try:
		...access shared resource
	finally:
		lock.release()

using context manager (with) which is easy to manage acquizition and relasing locked

example
	lock = threading.Lock()

	with lock:
		...acess share resource .


acquire() also take addtional argument boolean .
which is used if lock is acquired then perform some other operation .

if lock.acquire(false):
	lock acquired, do stuff with lock
else:
	... could not acquire lock , do other stuff .


there is other similar method also present

if locke.locked():
	do other stuff
else:
	lock.acquire()


Catagory of lock.

		Renterent Lock : RLock()

lock = threading.Lock()
lock.acqure()
lock.acqurie()  # this call will block

lock = threading.RLock()
lock.acquire()
lock.acquire()  # this call won't block





Semaphore is another synchronization technique which manages internal counter .


semaphore = threading.Semaphore(4)
semaphore.acquire() # decrement the counter .
... access the shared resource .
semphore.release()  # increase the coutner .


in common termnology semaphore maintain the set of permit .

if relase() function get called multiple times it counter may increase more than 4  .

to solve this problem  BoundedSemaphore introduced .
num_permit = 3
bounded_semaphore = threading.BoundedSemaphore(num_permit)




Threding Event :
one thread signal the event , other thread simply wait thread .


event = threading.Event()

# client thead can wait for the flag to be set
event.wait()

# server thread can set or reset it
event.set()    # set the flage to true .
event.clear()  # reset the flag to false.

example
	import threading
	import time
	event = threading.Event()
	def consumer():
		print("Consumer waiting for supplier permission")
		event.wait()
		print("Consumer given a permission ")


	def supplier():
		print("Setting the event")
		event.set()

	thread_1 = threading.Thread(target=consumer, args=())
	thread_2 = threading.Thread(target=supplier, args=())
	thread_1.start()
	time.sleep(4)
	thread_2.start()
	thread_1.join()
	thread_2.join()




threding.Condition:
It  just combine the property of lock and event .

like locke , it have
acquire() and release() method

like event
wait(), notify() , notify_all()


Example

cond = threading.Condition()
# consumer one itme
cond.acquire()
while not an_time_is_availaible():
    condi.wait()
get_an_avaialable_itme()
cond.release()

# Produce one item
make_an_item_available()
cond.notify()
cond.release()



If we don't want to involve event , condition then use queue.

Inter thread communication using Queues

put() : Puts an item into the queue
get() : Removes an item from queue.
task_done(): Marks an itme that was gotten from the queue as completed / processed .
It is used for joining main thread .

join() : Block until all the itmes in the queue have been processed .


Example :
from threading import Thread
from queue import Queues

def producer(queue):
	for i in range(10):
		time = make_an_itemavailaible()
		queue.put(item)

def consumer(queue):
	while True:
		item = queue.get()

		# do something with the item
		queue.task_done()  # mark the item as done . I

 queue = Queue()
 t1 = Thread(target=producer, args=(queue))
 t2 = Thread(target=consumer, args=(queue))
 t1.start()
 t2.start()


 Global Interpreter Lock
 The lock that prevent multiple native thread from executing Python code at same time .

 Python threading is not thread , this most majority of people think .
 To prevent race condition or thread interferance .


 That allow to write python code easy .
 It improve the single core performance .

GIL Workarournds : GIL -less Python Interpretor
		-- Jython
		-- IronPython

	Python Multiprocessing


Multiprocessing :

Process don't share memory and resources with other Process .
But some exception to where process ask operyting system to share resource some singnal or calls .

Adv :
Side Steps GIL
Less need for synchronization .
Can be paused or terminated .
More resilent

Dis:
Higher memory foot print .
Expensive context swithch .

Multiprocessing API
It is design to similar to threading API

e.g
import multiprocessing
import time
def do_some_work(value):
    time.sleep(10)
    print(value)

if __name__ == '__main__':
    p1 = multiprocessing.Process(target=do_some_work, args=(12,), daemon=True)
    p1.start()
    p1.join()


Pickling : Is the process where python ojbect hierchy is converted into bytestream .
Unpickling : Is reverse operation .


class multiprocessing.Process(group=None,
							  target=None,
							  name=None,
							  args=(),
							  kwargs=(),
							  daemon=None)


Daemon Process:
is a child process that does not prevent its parent process from exiting .

Terminating Process :
Process is killable by operating system API .

is_alive()
terminate()

p.exitcode : this attribute is used for determineing exit code .

process.terminate()
shared resources my be put in an inconsistent state .
Finally clauses and exit handler will not be run.

ProcessPools : An easy way to execute fixed pool of process .

multiprocessing.Pool class .


multiprocessing.Pool( num_of_process ,    # by defautl number of core	
					  initilizer,         # initilizer function
					  initargs ,          # picklable not required 
					  maxtaxperchild ,    # by default it is none , meaning that worker process alive
					                        as long as pool alive .
											if value set , after executing number of times the worker process get killed 
											and new process used .
											This ensure that long running process periodically release the resources 


											

The most common use case of pool .
pool.map(function , iterable object )

Example :
import multiprocessing

def do_work(data):
    return data**2

def start_process():
    print("Starting process",multiprocessing.current_process().name)

if __name__ == "__main__":
    process_count = multiprocessing.cpu_count() -1
    process_pool = multiprocessing.Pool(processes=process_count,
    initializer=start_process,maxtasksperchild=1)

    input_list = list(range(10))
    output  = process_pool.map(do_work,input_list)
    print("Output", output)
    process_pool.close()   # No more task submitted to pool .
    process_pool.join()   # wait for worker process to exit .

	
	There are async version of map funtion also availaible .
	
	join() method should be called after termination of pool or close() 
	


Pool.map_async(func, iterable , chunks , callback , ) return AsyncResult 
AsyncResult.get(timeout)  # Return a result when available
AsyncResult.successful

Similar function which take only one argument 
Pool.apply()
Poll.apply_async()

Inter Process Communication 
By default process don't share resources to each other .
If process want to share resources between two process need to use os supported communication channel.
if they want to exchange data between two channal.

Pipe, Queue.

Multiprocessing.Pipe : Represent two end connetion of the pipe

Pipe() : It will return two connection object . conn1 will send data to conn2
Pipe(True) : Connection is bidirection .  

conn.recv() : it will wait for data .

Pipe generally cause data corruption it to connection try to send data in one time .
For this Queue is more modern or secure way for inter process connection .

It also impliment all standard call which impliment in normal queue call .

threading.Queue : qsize , put , get , empty , full , task_done , join,
multiprocessing.Queue : qsize , put ,get ,empty , full


There also other queue which also impliment task_done , join 
multiprocessing.JoinableQueue 
										

Multiprocess.Lock
it used for accessing critical section of code between Process .

Process Synchronization module present all the method which is present in threading .
Lock, RLock , Semaphore, BoundedSeamphore , Event , Condition , Barier .

Abstracting Concurrency : 
In both threading and multiprocessing we used some kind of executor .
In case of IO boud task it uses theading executor to execute task and return output .
In case of CPU bound task it used process executor to execute task and return ouput .

we can convert this high descripion into interface .
in simple way

def do_something():
	print("Doing something ")
	return

executor = # new threadPool or processPool instance .
future_result = executor.submit(sayhello, "Tim")

for executing multiple function we may have map function .
future_result = executor.map(sayHello, ["Time","Chetan"])


Reason to switch Executors ?
  i) Task mix IO and CPU .
  ii) Change of Platform .
 
Executor API .
executor api is provided by "concurrent.future module" and exposes only 3 method.
submit , map and shutdown .

submit(fn, *args, **kwargs)  # Schedule a function to run . It return future object .
map(fn,iterable, timeout , chunksize=1)
shudown(wait=True) # stop accepting task and shutdown 

ThreadPoolExecutor(max_worker=None, thread_name_prefix="")

Example : 
from concurrent.futures import ThreadPoolExecutor
import threading
import random
import time

def do_some_work():
    sleep_time = random.randint(1, 9)
    time.sleep(sleep_time)
    print("I have complete the work ", threading.current_thread().name)
    return sleep_time

if __name__ == '__main__':
    pool_exec = ThreadPoolExecutor(thread_name_prefix="My_Context")
    f1 = pool_exec.submit(do_some_work)
    f2 = pool_exec.submit(do_some_work)

    print("Those are thread pool executor result ")
    try:
        print(f1.result())
        print(f2.result())
    except Exception as e:
        print(e)

#### For cpu bound task we should write the code in that way .
from concurrent.futures import ProcessPoolExecutor
import threading
import random
import time

def do_some_work():
    sleep_time = random.randint(1, 9)
    time.sleep(sleep_time)
    print("I have complete the work ", threading.current_thread().name)
    return sleep_time

if __name__ == '__main__':
    pool_exec = ProcessPoolExecutor()
    f1 = pool_exec.submit(do_some_work)
    f2 = pool_exec.submit(do_some_work)

    print("Those are thread pool executor result ")
    try:
        print(f1.result())
        print(f2.result())
    except Exception as e:
        print(e)

## Future Object 
An object that act as proxy for a result that is yet to be computed .

Future object is based for Asynchronous programing , 
Internally pool_exec.submit or pool.map pool.map_async .. internally uses future object to accumplish this .

future_obj = exector.submit(fun,*args, **kwargs)
... do something 
result = future_obj.result()

executor is Actor  ===> where we submit the some task .

futur_obj.cancel : it will cancel the execution. Return True if successful .
done() : # Returns true if completed or canceled .

exception(timeout=None) # Returns the exception if any .

add_done_callback(fn) # attaches function to be called on completion or cancellation .
multiple callback function can be added , it will get called in which order they get added .

When we have future object we may need to wait when all are completed .
concurrent.futures.wait(fs,timeout=None,return_when=ALL_COMPLETED)
concurrent.futures.as_completed(fs,timeout=None) # It return iterator which contain any future object completed .

Single Thereaded Asynchronous : 
we can achive interleaving execution of task . 

Thread ---> T1 , T4 , T3 ......

It execute T1 for some time. then it interleave the executin and execute T4 . 
after some time it then execute T1 again .   

It best suited for IO bound task . 

Implimentation of single threaded asynchroni is generally depend on event driven archtecture .
Is software design that orchestrates behaviour around the production , dectection and consumption of events .

Event Loop :  is responsible of getting items from an event queue and handling it .

Event can be : change of File state , Timeout occuring , New data at network socket .

In nodejs programing , event loop is invisible to the programer because it
 implimented under the hood of v8 engine or libuv library

But in case of python it is implicitly implimented .

#### Coperative multitasking with Event loop and Coroutines . 
asyncio.get_event_loop()  # It return an object of abstract event loop .

AbstractEventLoop.run_forever()
AbstractEventLoop.run_unitl_complete()

AbastractEventLoop.stop()
AbastractEventLoop.close()

Cooperative Multitasking
-- Tasks suspend themselves to allow other run .
-- Event loop resume the task when the IO operation complete .
-- Task ==> Corouting  comming . 

There are two constuct in python which called coroutine .
1. Coroutine Function 
2. Coroutine Object 

Coroutine Function :
	It is a special function 
    It give up the control to the caller without loosing its step .

Exaple of Coroutine function are 

import asyncio
async def say_hello():
    print("He bro I am learning asynchronous Programing ")


event_loop = asyncio.get_event_loop()
event_loop.run_until_complete(say_hello())
event_loop.close()

async keyword conver say_hello function to coroutine function .


In python 3.4 generator are used to impliment courotine function .

yield from 
@asyncio.coroutine 

Python 3.5 
await and async are implimented 

CoroutineObject = CoroutineFunction()

Executing corutine function doesnt' mean that executing the instruction .
Rater it return the coroutine object . 
It coroutine object need to wrap into future object and passed it into event loop. 
run_unitl_complete method internally wrap the function into future object and pass it to the event loop.

Asyncio Future . 
Future : It manages the execution and represent the eventual result of computation .

Non-Blocking : 
		asyncio.Future 
			.result()
			.exception()

Blocking :
	concurrent.future.Future
		.result()
		.exception()

waiting for Future to complete .
	await future # pause the execution untile future is done .

if you are outside the function . 
loop.run_unitl_complete(future) . It will wait for future to be complete .Loop stop after future complete .

Task : Is a subclass of Future that is used to wrap and manage the execution of a coroutine in event loop . 

Creating a Task 
asyncio.ensure_future(coro_or_future, * ,loop=None)
this function wrap coroutine into task . 

Coroutine Chaning : 
A coroutine awaiting another coroutine .

Parallel Execution of task : 
coroutine asyncio.wait(futures, *,
						loop,
						timeout,
						return_when=ALL_COMPLETED)

Return (DONE_FUTURE,PENDING_FUTURE)


if you want to only wait for only one fuction call , then we can use this
asyncio.wait_for(future, timeout ,loop=None)

example : 
		import asyncio


		async def get_item(num):
			print("get_item")
			await asyncio.sleep(num)
			return "item" + str(num)


		async def get_items(num):
			print("Get items")
			items_cors = [get_item(i) for i in range(num)]

			print("Waiting for task to complete")
			complete, pending = await asyncio.wait(items_cors, timeout=2)

			if pending:
				print("Canceling remaining task ")
				for t in pending:
					print(type(t))
					t.cancel()

			result = [t.result() for t in complete]
			print("Result : {!r}".format(result))


		loop = asyncio.get_event_loop()

		try:
			loop.run_until_complete(get_items(4))
		finally:
			loop.close()


						

Example 2 : 
try : 
	result = asyncio.wait_for(task, 300)
except asyncio.TimeoutError:
	print("Task did not complete in 30 seconds so it was canceled ")

if we want to yield the task as completed on that case use this method .
asyncio.as_completed(fs,*,loop=None,timeout=None)

Example : 
for task in asyncio.as_completed(tasks):
	result = await task 

asyncio.gather(*cors, loop=None, return_exception=False)  
return_exception=False : It will propogate the exception .
return_exception=True : return exception is treated as result .



https://docs.python.org/dev/library/asyncio-subprocess.html#asyncio-subprocess




coroutine : 

Python’s coroutines, special functions that give up control to the caller without losing their state.
or 
Generalize subrouting with non-preemptive multitasking by allowing execution suspended and resumed .

subroutine : is sequence of program instruction that perform specfict , packaged as unit . Function .

non-preemptive multitasking (cooperative multitasking) : A style of computer multitasking in which operating system 
never initiates a context switch from running process . Istead process volunerly yield control 

Coroutine very similar to threads , However , couroutines are cooperative multitasked , where
threads are typically preemitively multitasked . This means that thread provide concurrency but not parallellism .

Adv Coroutine: 
switching between coroutine need not involve any system calls or any blocking calls whatsoever . 

Generators, also known as semicoroutines.

A Task is a subclass of Future that knows how to wrap and manage the execution for a coroutine. 
Tasks can be scheduled with the event loop to run when the resources they need are available,
 and to produce a result that can be consumed by other coroutines.


Scheduling a Callback “Soon”
loop.call_soon(wrapped, 2)

Scheduling a Callback with a Delay
To postpone a callback until some time in the future, use call_later(). 
The first argument is the delay in seconds and the second argument is the callback.
loop.call_later(0.1, callback, 2)


Scheduling a Callback for a Specific Time
now = loop.time()
loop.call_at(now + 0.1, callback, 2, loop) 
loop.call_at(monolitic_time , callback,parameter , parameter) 


























